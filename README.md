# Deep-Learning

1. Implementating and Training a neural network from scratch in Python for the MNIST dataset (no PyTorch). (SGD)
      
       The neural network is trained on the Training Set using Stochastic Gradient Descent Algorithm
       
       Foward and Backward Propogation algorithm is used with sigmoid function for nonlinearity
       
      
2. Implement and train a single-layer convolution neural network from scratch in Python for the MNIST dataset (no PyTorch).
      
       You should write your own code for convolutions (e.g., do not use SciPy's convolution function). 
       
       The convolution network should have a single hidden layer with multiple channels. 
       
       It should achieve at least 94% accuracy on the Test Set (it would be good to achieve 98% test accuracy though).


3. Train a deep convolution network on a GPU with PyTorch for the CIFAR10 dataset. 
      
       The convolution network should use (A) dropout, (B) trained with RMSprop or ADAM, and (C) data augmentation. 
       
       Compare dropout test accuracy (i) using the heuristic prediction rule and (ii) Monte Carlo simulation. 
       
       The model should achieve 80-90% Test Accuracy.
       

4. Build very deep convolutional networks using Residual Networks (ResNets).


5. Work with the Large Movie Review Dataset and provide an understanding of how Deep Learning is used within the field of Natural Language Processing (NLP). 
       
       The original paper published in 2011 discussing the dataset and a wide variety of techniques can be found here. 
       
       Train models to detect the sentiment of written text. 
       
       More specifically, try to feed a movie review into a model and have it predict whether it is a positive or negative movie review.
